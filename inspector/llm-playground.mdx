---
title: "LLM Playground"
description: "Test your MCP server with real AI models using the integrated chat playground"
---

The LLM Playground is one of MCPJam Inspector's most powerful features, allowing you to test how your MCP server behaves with real AI models in a realistic chat environment. This helps you validate that your tools and resources work correctly in actual usage scenarios.

## Supported AI Providers

### OpenAI
Test with industry-leading models including GPT-4 and GPT-3.5:

- **GPT-4**: Best for complex reasoning and tool usage
- **GPT-3.5 Turbo**: Fast and cost-effective for most testing
- **GPT-4 Turbo**: Optimized for speed and efficiency

### Anthropic Claude
Leverage Claude's advanced reasoning capabilities:

- **Claude 3 Opus**: Most capable model for complex tasks
- **Claude 3 Sonnet**: Balanced performance and speed
- **Claude 3 Haiku**: Fast responses for simple interactions

### DeepSeek AI
Test with specialized coding and reasoning models:

- **DeepSeek R1**: Optimized for reasoning and problem-solving
- **DeepSeek Coder**: Specialized for code-related tasks
- **Custom Models**: Support for custom DeepSeek deployments

### Ollama (Local Models)
Run models locally for privacy and offline testing:

- **Llama 3.2**: Versatile general-purpose model
- **Code Llama**: Specialized for coding tasks  
- **Mistral**: Efficient European AI model
- **Custom Models**: Any Ollama-compatible model

<img src="/images/model-selection.png" alt="AI Model Selection" />

## Setting Up AI Providers

### OpenAI Configuration

1. **Get API Key**: Visit [OpenAI's API page](https://platform.openai.com/api-keys)
2. **Add to Inspector**: Go to Settings â†’ AI Providers
3. **Enter Credentials**:
   ```json
   {
     "provider": "openai",
     "apiKey": "sk-...",
     "organization": "org-..." // Optional
   }
   ```

### Claude Configuration

1. **Get API Key**: Visit [Anthropic Console](https://console.anthropic.com/)
2. **Configure in Inspector**:
   ```json
   {
     "provider": "anthropic", 
     "apiKey": "sk-ant-...",
     "version": "2023-06-01"
   }
   ```

### DeepSeek Setup

1. **Create Account**: Sign up at [DeepSeek Platform](https://platform.deepseek.com/)
2. **Configure Provider**:
   ```json
   {
     "provider": "deepseek",
     "apiKey": "sk-...",
     "baseURL": "https://api.deepseek.com"
   }
   ```

### Ollama Local Setup

Quick setup with CLI:
```bash
# Launch Inspector with Ollama
npx @mcpjam/inspector@latest --ollama llama3.2
```

Manual setup:
```bash
# Install and start Ollama
ollama pull llama3.2
ollama serve
```

Then configure in Inspector:
```json
{
  "provider": "ollama",
  "baseURL": "http://localhost:11434",
  "model": "llama3.2"
}
```

## Using the Playground

### Basic Chat Interface

<img src="/images/playground-interface.png" alt="LLM Playground Interface" />

The playground provides a familiar chat interface where you can:

1. **Select Model**: Choose from configured AI providers
2. **Set Parameters**: Adjust temperature, max tokens, etc.
3. **Chat Naturally**: Interact as you would with any AI assistant
4. **Monitor Tools**: See when and how your MCP tools are used

### Testing Your MCP Tools

When you chat with the AI, it automatically has access to your connected MCP server's tools:

```
You: "What's the weather like in San Francisco?"